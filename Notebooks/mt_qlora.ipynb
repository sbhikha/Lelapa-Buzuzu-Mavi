{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma 2 QLoRA Adapter Training Script\n",
    "\n",
    "This script uses HF Transformers and PEFT to train QLoRA adapters for Gemma 2\n",
    "\n",
    "Script Overview:\n",
    "- Loads Base Gemma 2 model with double quantisation\n",
    "- Loads finetuning datasets from teh HF Hub\n",
    "- Initialises new PEFT Model for training using QLoRA adapters\n",
    "- Configures Supervised Finetuning Trainer\n",
    "- Trains QLoRA Adapter and saves to the HF Hub\n",
    "\n",
    "The adapters produced by this script are held on the HF Hub and downloadable at https://huggingface.co/sbhikha\n",
    "\n",
    "List of trained adapters:\n",
    "- Working adapters:\n",
    "    - ENG Adapter: \"sbhikha/Gemma9B_Inkuba_LoRa_eng_v1\"\n",
    "    - XHO Adapter: \"sbhikha/Gemma9B_Inkuba_LoRa_xho_v1\"\n",
    "    - ZUL Adapter\" \"sbhikha/Gemma9B_Inkuba_LoRa_zul_v1\"\n",
    "    - COMBINED Adapter: \"sbhikha/Gemma9B_Inkuba_LoRa_combined_v1\"\n",
    "- Failed:\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_adapter_v1 (Trained on Sliced Datasets)\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_eng_v2\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_eng_v3\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_zul_v3\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_xho_v4\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_combined_v5\n",
    "    - sbhikha/Gemma9B_Inkuba_LoRA_combined_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 27 07:34:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 58%   44C    P8             55W /  370W |    1462MiB /  10240MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3964    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A      7188    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      8360    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      9884    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     11916    C+G   C:\\Program Files\\NordVPN\\NordVPN.exe        N/A      |\n",
      "|    0   N/A  N/A     12148    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     12756    C+G   ...ta\\Local\\Programs\\Notion\\Notion.exe      N/A      |\n",
      "|    0   N/A  N/A     13588    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A     14772    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A     15004    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     16800    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     17504    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     19204    C+G   ...Software\\Current\\LogiOptionsMgr.exe      N/A      |\n",
      "|    0   N/A  N/A     19356    C+G   ...ns\\Software\\Current\\LogiOverlay.exe      N/A      |\n",
      "|    0   N/A  N/A     20012    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     21452    C+G   ...3d8bbwe\\dotnet\\PAD.Console.Host.exe      N/A      |\n",
      "|    0   N/A  N/A     22612    C+G   ...air\\Corsair iCUE5 Software\\iCUE.exe      N/A      |\n",
      "|    0   N/A  N/A     24504    C+G   ...\\Local\\slack\\app-4.42.120\\slack.exe      N/A      |\n",
      "|    0   N/A  N/A     24996    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     25376    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A     26128    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     26696    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     28108    C+G   ...AppData\\Roaming\\Spotify\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     29612    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     30304    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     30564    C+G   ...\\Local\\slack\\app-4.42.120\\slack.exe      N/A      |\n",
      "|    0   N/A  N/A     31532    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     41268    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     41644    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     59588    C+G   ...255.0_x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# ! pip install -U bitsandbytes\n",
    "# ! pip install huggingface_hub\n",
    "# ! pip install transformers\n",
    "# ! pip install pandas\n",
    "# ! pip install accelerate\n",
    "# ! pip install --upgrade torch torchvision torchaudio\n",
    "# ! pip install ipython\n",
    "# ! pip install peft\n",
    "# ! pip install trl\n",
    "# ! pip install datasets\n",
    "# ! pip install flash-attn --no-build-isolation\n",
    "# ! pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\Sandil\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Log into HF Hub\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Gemma 2 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffef732073b24ac9adeec473ba80c655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    quantization_config=double_quant_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='eager'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><|system|> You are a machine translation assistant. Your task is to translate the user's input sentence from isiZulu into English.<|end|> <|user|> Izibalo zangaphakathi: Isidingo sezibalo sizokwanda njengoba kugxila intandoyeningi <|end|> <|assistant|> Internal calculations: The calculation process will increase as the complexity grows. <|end|><end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate the following [SENTENCE] from {SRC_LANG} to {TGT_LANG}. Return only the [TRANSLATION] in {TGT_LANG}. [SENTENCE] {SRC_LANG}: {SRC_SENT}. [TRANSLATION] {TGT_LANG}:\"\n",
    "prompt_2 = \"<|system|> You are a machine translation assistant. Your task is to translate the user's input sentence from {SRC_LANG} into {TGT_LANG}.<|end|> <|user|> {SRC_SENT} <|end|> <|assistant|>\"\n",
    "tgt_lang = \"English\"\n",
    "src_lang = \"isiZulu\"\n",
    "src_sent = \"Izibalo zangaphakathi: Isidingo sezibalo sizokwanda njengoba kugxila intandoyeningi\"\n",
    "\n",
    "input_ids = tokenizer(prompt_2.format(SRC_LANG=src_lang, TGT_LANG=tgt_lang, SRC_SENT=src_sent), return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "translation = tokenizer.decode(outputs[0])\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Weights & Biases\n",
    "- Used for keeping track of training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msandil-bhikha\u001b[0m (\u001b[33msandil-bhikha-university-of-liverpool\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20240917_113019-efgg1jnn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29/runs/efgg1jnn' target=\"_blank\">ethereal-terrain-4</a></strong> to <a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29' target=\"_blank\">https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29/runs/efgg1jnn' target=\"_blank\">https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29/runs/efgg1jnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_token = 'acd48cc467b99d27cd51968cc0ece2ceb9dde7f1'\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-9b-it on Inkuba Eng (V2)', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Finetuning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "DATA = \"sbhikha/Inkuba_{lang}_{set}_instruction_tuning\"\n",
    "\n",
    "eng_train = load_dataset(DATA.format(lang=\"english\", set=\"train\"))\n",
    "eng_dev = load_dataset(DATA.format(lang=\"english\", set=\"dev\"))\n",
    "\n",
    "zul_train = load_dataset(DATA.format(lang=\"isizulu\", set=\"train\"))\n",
    "zul_dev = load_dataset(DATA.format(lang=\"isizulu\", set=\"dev\"))\n",
    "\n",
    "xho_train = load_dataset(DATA.format(lang=\"xhosa\", set=\"train\"))\n",
    "xho_dev = load_dataset(DATA.format(lang=\"xhosa\", set=\"dev\"))\n",
    "\n",
    "train_data = concatenate_datasets([eng_train[\"train\"], zul_train[\"train\"], xho_train[\"train\"]])\n",
    "dev_data = concatenate_datasets([eng_dev[\"train\"], zul_dev[\"train\"], xho_dev[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_input', 'output', '__index_level_0__'],\n",
       "    num_rows: 58851\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n",
    "# dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT (QLoRa) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PEFT and prepare model\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Find trainable modules\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit \n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'k_proj', 'gate_proj', 'down_proj', 'q_proj', 'o_proj', 'up_proj']\n"
     ]
    }
   ],
   "source": [
    "# Print list of trainable modules\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRa hyperparameters\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Show PEFT Model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 216072192 | total: 9457778176 | Percentage: 2.2846%\n"
     ]
    }
   ],
   "source": [
    "# Print adapter % of model (trainable parameters)\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f778ac97cf4a6b8862466f819e05fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d4584d85f3400d9dcb04d92b7fe6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:407: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "folder = \"Gemma9B_QloRA_Adapter\"\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create Supervised Fine Tuning Job\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=dev_data,\n",
    "    dataset_text_field=\"text_input\",\n",
    "    peft_config=lora_config,\n",
    "    args=TrainingArguments(             # Hyperparameters\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=3,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"Gemma9B_QLoRA Adapter\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        report_to=\"wandb\"\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:22:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.943896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c3ec6ee60347da81bf6828ee19a82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/864M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203fb66ee9d549b19df99e892647ee4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▇█▃▄▂▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▃████▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>███▄▂▂▂▂▁▁▁▂▁▂▁▁▁▂▂▁▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.9439</td></tr><tr><td>eval/runtime</td><td>4397.3963</td></tr><tr><td>eval/samples_per_second</td><td>11.478</td></tr><tr><td>eval/steps_per_second</td><td>1.435</td></tr><tr><td>total_flos</td><td>5950794723643392.0</td></tr><tr><td>train/epoch</td><td>0.02719</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>0.40107</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.8516</td></tr><tr><td>train_loss</td><td>1.08859</td></tr><tr><td>train_runtime</td><td>4937.4787</td></tr><tr><td>train_samples_per_second</td><td>0.324</td></tr><tr><td>train_steps_per_second</td><td>0.02</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-terrain-4</strong> at: <a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29/runs/efgg1jnn' target=\"_blank\">https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29/runs/efgg1jnn</a><br/> View project at: <a href='https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29' target=\"_blank\">https://wandb.ai/sandil-bhikha-university-of-liverpool/Fine-tune%20Gemma-2-9b-it%20on%20Inkuba%20ENG%20%28V2%29</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240917_113019-efgg1jnn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(\"sbhikha/Gemma9B_Inkuba_LoRa_new\")\n",
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT_HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
